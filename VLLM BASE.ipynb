{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Установка необходимых пакетов\n",
    "!pip install vllm transformers datasets peft torch accelerate\n",
    "!pip install bitsandbytes  # для 4-bit/8-bit квантования\n",
    "\n",
    "# Импорт библиотек\n",
    "from vllm import LLM, SamplingParams\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForCausalLM, \n",
    "    TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "from datasets import Dataset\n",
    "import torch\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Инициализация модели для инференса\n",
    "model_name = \"mistralai/Mistral-7B-Instruct-v0.2\"  # пример модели\n",
    "\n",
    "# Параметры инициализации vLLM\n",
    "llm = LLM(\n",
    "    model=model_name,\n",
    "    tensor_parallel_size=1,           # Количество GPU для тензорного параллелизма\n",
    "    gpu_memory_utilization=0.9,       # Использование памяти GPU (0.0-1.0)\n",
    "    max_model_len=4096,               # Максимальная длина контекста\n",
    "    quantization=\"awq\",               # Квантование: \"awq\", \"squeezellm\", None\n",
    "    trust_remote_code=True,           # Доверять пользовательскому коду\n",
    "    enforce_eager=True,               # Для отладки - отключает оптимизации\n",
    ")\n",
    "\n",
    "# Параметры семплирования\n",
    "sampling_params = SamplingParams(\n",
    "    temperature=0.7,                  # Случайность (0.0-1.0)\n",
    "    top_p=0.9,                        # Nucleus sampling\n",
    "    top_k=50,                         # Top-k sampling\n",
    "    max_tokens=512,                   # Максимальная длина генерируемого текста\n",
    "    presence_penalty=0.0,             # Штраф за повторяющиеся токены\n",
    "    frequency_penalty=0.0,            # Штраф за частые токены\n",
    "    stop=[\"</s>\", \"\\n\\n\"],            # Стоп-токены\n",
    "    n=1,                              # Количество вариантов генерации\n",
    "    best_of=3,                        # Лучший из n вариантов\n",
    ")\n",
    "\n",
    "# Подготовка промптов\n",
    "prompts = [\n",
    "    \"Объясни концепцию машинного обучения:\",\n",
    "    \"Напиши код для сортировки пузырьком на Python:\",\n",
    "    \"Какие основные преимущества трансформеров в NLP?\"\n",
    "]\n",
    "\n",
    "# Запуск инференса\n",
    "outputs = llm.generate(prompts, sampling_params)\n",
    "\n",
    "# Обработка результатов\n",
    "for i, output in enumerate(outputs):\n",
    "    prompt = prompts[i]\n",
    "    generated_text = output.outputs[0].text\n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    print(f\"Generated: {generated_text}\")\n",
    "    print(f\"Tokens generated: {len(output.outputs[0].token_ids)}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Потоковая генерация для длинных текстов\n",
    "def stream_generation(prompts, sampling_params):\n",
    "    for output in llm.generate(prompts, sampling_params, use_tqdm=True):\n",
    "        # Реализация потоковой обработки\n",
    "        if hasattr(output, 'outputs') and output.outputs:\n",
    "            yield output.outputs[0].text\n",
    "\n",
    "# Пример использования потоковой обработки\n",
    "streaming_prompts = [\"Расскажи подробно о глубоком обучении:\"]\n",
    "for result in stream_generation(streaming_prompts, sampling_params):\n",
    "    print(\"Получен фрагмент:\", result[:100] + \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Формат данных для instruction tuning\n",
    "def prepare_instruction_data():\n",
    "    # Пример данных - замените на свои\n",
    "    instructions = [\n",
    "        {\n",
    "            \"instruction\": \"Объясни концепцию\",\n",
    "            \"input\": \"нейронные сети\",\n",
    "            \"output\": \"Нейронные сети - это вычислительные системы...\"\n",
    "        },\n",
    "        {\n",
    "            \"instruction\": \"Напиши код\", \n",
    "            \"input\": \"функция сложения на Python\",\n",
    "            \"output\": \"def add(a, b):\\n    return a + b\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    formatted_data = []\n",
    "    for item in instructions:\n",
    "        # Форматирование в чат-формат\n",
    "        text = f\"<s>[INST] {item['instruction']}: {item['input']} [/INST] {item['output']} </s>\"\n",
    "        formatted_data.append({\"text\": text})\n",
    "    \n",
    "    return Dataset.from_list(formatted_data)\n",
    "\n",
    "# Создание dataset\n",
    "dataset = prepare_instruction_data()\n",
    "train_dataset = dataset.train_test_split(test_size=0.1)['train']\n",
    "eval_dataset = dataset.train_test_split(test_size=0.1)['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Параметры LoRA\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,      # Тип задачи - causal language modeling\n",
    "    inference_mode=False,              # Режим обучения\n",
    "    r=16,                              # Rank матриц LoRA\n",
    "    lora_alpha=32,                     # Коэффициент масштабирования\n",
    "    lora_dropout=0.1,                  # Dropout для LoRA\n",
    "    target_modules=[\n",
    "        \"q_proj\",      # Проекции запросов\n",
    "        \"k_proj\",      # Проекции ключей  \n",
    "        \"v_proj\",      # Проекции значений\n",
    "        \"o_proj\",      # Выходные проекции\n",
    "        \"gate_proj\",   # Gate проекции (для Mistral)\n",
    "        \"up_proj\",     # Up проекции\n",
    "        \"down_proj\",   # Down проекции\n",
    "    ],\n",
    "    bias=\"none\",                       # Обработка bias: \"none\", \"all\", \"lora_only\"\n",
    ")\n",
    "\n",
    "# Загрузка базовой модели\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.bfloat16,        # Тип данных\n",
    "    device_map=\"auto\",                 # Автоматическое распределение по GPU\n",
    "    trust_remote_code=True,\n",
    "    load_in_4bit=True,                 # 4-bit квантование для экономии памяти\n",
    ")\n",
    "\n",
    "# Применение LoRA\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()     # Покажет количество обучаемых параметров"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Параметры обучения\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./lora-finetuned-model\",  # Директория для сохранения\n",
    "    per_device_train_batch_size=4,        # Batch size на устройство\n",
    "    per_device_eval_batch_size=4,\n",
    "    gradient_accumulation_steps=4,        # Накопление градиентов\n",
    "    learning_rate=2e-4,                   # Скорость обучения для LoRA\n",
    "    num_train_epochs=3,                   # Количество эпох\n",
    "    logging_steps=10,                     # Шаг логирования\n",
    "    eval_steps=100,                       # Шаг оценки\n",
    "    save_steps=500,                       # Шаг сохранения\n",
    "    warmup_steps=100,                     # Шаги прогрева\n",
    "    evaluation_strategy=\"steps\",          # Стратегия оценки\n",
    "    save_strategy=\"steps\",                # Стратегия сохранения\n",
    "    load_best_model_at_end=True,          # Загрузка лучшей модели\n",
    "    metric_for_best_model=\"eval_loss\",    # Метрика для лучшей модели\n",
    "    greater_is_better=False,\n",
    "    fp16=True,                            # Использование mixed precision\n",
    "    report_to=\"none\",                     # Отключение внешних логгеров\n",
    "    ddp_find_unused_parameters=False,\n",
    ")\n",
    "\n",
    "# Data collator для language modeling\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token  # Установка pad токена\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False,  # Не использовать masked language modeling\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Создание Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "# Запуск обучения\n",
    "trainer.train()\n",
    "\n",
    "# Сохранение модели\n",
    "trainer.save_model()\n",
    "tokenizer.save_pretrained(\"./lora-finetuned-model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### peft inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "\n",
    "# Загрузка базовой модели\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "# Загрузка адаптеров LoRA\n",
    "model = PeftModel.from_pretrained(\n",
    "    base_model,\n",
    "    \"./lora-finetuned-model\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "# Переход в режим инференса\n",
    "model.eval()\n",
    "\n",
    "# Инференс с адаптированной моделью\n",
    "def generate_with_lora(prompt):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=512,\n",
    "            temperature=0.7,\n",
    "            do_sample=True,\n",
    "            top_p=0.9,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Тестирование\n",
    "test_prompt = \"Объясни принцип работы внимания в трансформерах:\"\n",
    "result = generate_with_lora(test_prompt)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### слияние весов и vllm инференс"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Слияние адаптеров LoRA с базовой моделью\n",
    "model = PeftModel.from_pretrained(\n",
    "    base_model,\n",
    "    \"./lora-finetuned-model\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "# Слияние весов\n",
    "merged_model = model.merge_and_unload()\n",
    "\n",
    "# Сохранение объединенной модели\n",
    "merged_model.save_pretrained(\"./merged-model\")\n",
    "tokenizer.save_pretrained(\"./merged-model\")\n",
    "\n",
    "# Загрузка в vLLM для быстрого инференса\n",
    "merged_llm = LLM(\n",
    "    model=\"./merged-model\",\n",
    "    tensor_parallel_size=1,\n",
    "    gpu_memory_utilization=0.8,\n",
    "    max_model_len=4096,\n",
    ")\n",
    "\n",
    "# Инференс с объединенной моделью\n",
    "prompts = [\"Объясни трансформеры простыми словами:\"]\n",
    "sampling_params = SamplingParams(temperature=0.7, max_tokens=256)\n",
    "outputs = merged_llm.generate(prompts, sampling_params)\n",
    "\n",
    "for output in outputs:\n",
    "    print(output.outputs[0].text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
