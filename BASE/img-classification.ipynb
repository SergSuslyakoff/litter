{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10899727,"sourceType":"datasetVersion","datasetId":6774035}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\n\nimport os\nimport random\nfrom PIL import Image\nfrom tqdm import tqdm, tqdm_notebook\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\n\nfrom transformers import Trainer, TrainingArguments\nfrom transformers import AutoImageProcessor, AutoModelForImageClassification\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nos.environ[\"WANDB_MODE\"] = \"disabled\"\nos.environ[\"WANDB_DISABLED\"] = \"true\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-03T09:34:02.470547Z","iopub.execute_input":"2025-03-03T09:34:02.470842Z","iopub.status.idle":"2025-03-03T09:34:24.363368Z","shell.execute_reply.started":"2025-03-03T09:34:02.470818Z","shell.execute_reply":"2025-03-03T09:34:24.362492Z"}},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":"## Preparing data for training","metadata":{}},{"cell_type":"code","source":"model_path = 'google/vit-base-patch16-224'\nprocessor = AutoImageProcessor.from_pretrained(model_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-03T09:34:24.364511Z","iopub.execute_input":"2025-03-03T09:34:24.365178Z","iopub.status.idle":"2025-03-03T09:34:25.907844Z","shell.execute_reply.started":"2025-03-03T09:34:24.365126Z","shell.execute_reply":"2025-03-03T09:34:25.906964Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/160 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"50dd16cb4c7c4f37bed9bee179590095"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/69.7k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d3d2c7745cc5424c90221ca848b10220"}},"metadata":{}},{"name":"stderr","text":"Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"img = Image.open('/kaggle/input/image-classification/train/train/cactus/image_00188.jpg')\nprocessor(img, return_tensors='pt')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-03T09:34:25.909240Z","iopub.execute_input":"2025-03-03T09:34:25.909491Z","iopub.status.idle":"2025-03-03T09:34:26.045053Z","shell.execute_reply.started":"2025-03-03T09:34:25.909463Z","shell.execute_reply":"2025-03-03T09:34:26.044210Z"}},"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"{'pixel_values': tensor([[[[-0.0353, -0.0275, -0.0196,  ...,  0.2549,  0.2549,  0.2549],\n          [-0.0353, -0.0275, -0.0196,  ...,  0.2549,  0.2549,  0.2549],\n          [-0.0353, -0.0275, -0.0196,  ...,  0.2549,  0.2549,  0.2549],\n          ...,\n          [-0.1686, -0.1686, -0.1686,  ...,  0.1216,  0.1216,  0.1216],\n          [-0.1765, -0.1765, -0.1765,  ...,  0.1137,  0.1137,  0.1137],\n          [-0.1843, -0.1843, -0.1843,  ...,  0.1059,  0.1059,  0.1059]],\n\n         [[-0.0510, -0.0431, -0.0353,  ...,  0.2549,  0.2549,  0.2549],\n          [-0.0510, -0.0431, -0.0353,  ...,  0.2549,  0.2549,  0.2549],\n          [-0.0510, -0.0431, -0.0353,  ...,  0.2549,  0.2549,  0.2549],\n          ...,\n          [-0.1843, -0.1843, -0.1843,  ...,  0.1216,  0.1216,  0.1216],\n          [-0.1922, -0.1922, -0.1922,  ...,  0.1137,  0.1137,  0.1137],\n          [-0.2000, -0.2000, -0.2000,  ...,  0.1059,  0.1059,  0.1059]],\n\n         [[ 0.0510,  0.0588,  0.0667,  ...,  0.3490,  0.3490,  0.3490],\n          [ 0.0510,  0.0588,  0.0667,  ...,  0.3490,  0.3490,  0.3490],\n          [ 0.0510,  0.0588,  0.0667,  ...,  0.3490,  0.3490,  0.3490],\n          ...,\n          [-0.0745, -0.0745, -0.0745,  ...,  0.2157,  0.2157,  0.2157],\n          [-0.0824, -0.0824, -0.0824,  ...,  0.2078,  0.2078,  0.2078],\n          [-0.0902, -0.0902, -0.0902,  ...,  0.2000,  0.2000,  0.2000]]]])}"},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"class_to_label = {\n    0: 'cactus',\n    1: 'fern',\n    2: 'rose',\n    3: 'sunflower',\n    4: 'tulip'\n}\n\nlabel_to_class = {name: idx for idx, name in class_to_label.items()}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-03T09:34:26.046102Z","iopub.execute_input":"2025-03-03T09:34:26.046371Z","iopub.status.idle":"2025-03-03T09:34:26.050278Z","shell.execute_reply.started":"2025-03-03T09:34:26.046353Z","shell.execute_reply":"2025-03-03T09:34:26.049309Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"class ImgDataset(Dataset):\n    def __init__(self, img_path, processor):\n        self.paths = []\n        self.labels = []\n        for label in os.listdir(img_path):\n            label_path = os.path.join(img_path, label)\n            for image in os.listdir(label_path):\n                self.paths.append(os.path.join(label_path, image))\n                self.labels.append(label_to_class[label])\n        self.processor = processor\n\n    def __len__(self):\n        return len(self.labels)\n\n    def __getitem__(self, idx):      \n        image = Image.open(self.paths[idx])\n        item = self.processor(image, return_tensors='pt')\n        item[\"pixel_values\"] = item[\"pixel_values\"].squeeze(0)\n        item[\"labels\"] = self.labels[idx]\n        return item","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-03T09:34:26.051119Z","iopub.execute_input":"2025-03-03T09:34:26.051405Z","iopub.status.idle":"2025-03-03T09:34:26.065034Z","shell.execute_reply.started":"2025-03-03T09:34:26.051377Z","shell.execute_reply":"2025-03-03T09:34:26.064210Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"train_dataset = ImgDataset('/kaggle/input/image-classification/train/train', processor)\nlen(train_dataset)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-03T09:34:26.065963Z","iopub.execute_input":"2025-03-03T09:34:26.066314Z","iopub.status.idle":"2025-03-03T09:34:26.145296Z","shell.execute_reply.started":"2025-03-03T09:34:26.066283Z","shell.execute_reply":"2025-03-03T09:34:26.144424Z"}},"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"2400"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"train_dataset[0]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-03T09:34:38.118813Z","iopub.execute_input":"2025-03-03T09:34:38.119085Z","iopub.status.idle":"2025-03-03T09:34:38.141840Z","shell.execute_reply.started":"2025-03-03T09:34:38.119061Z","shell.execute_reply":"2025-03-03T09:34:38.140989Z"}},"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"{'pixel_values': tensor([[[-0.6392, -0.6863, -0.7020,  ..., -0.8980, -0.8667, -0.8588],\n         [-0.6157, -0.6549, -0.6941,  ..., -0.9137, -0.8824, -0.8902],\n         [-0.6392, -0.6157, -0.6078,  ..., -0.9686, -0.9608, -0.8980],\n         ...,\n         [-0.3804, -0.3490, -0.2706,  ...,  0.4588,  0.4588,  0.4039],\n         [-0.3647, -0.2863, -0.2000,  ...,  0.5529,  0.5451,  0.4824],\n         [-0.3725, -0.3098, -0.2000,  ...,  0.4980,  0.5765,  0.5686]],\n\n        [[ 0.0980,  0.0745,  0.0902,  ...,  0.1608,  0.1686,  0.1608],\n         [ 0.1529,  0.1294,  0.1059,  ...,  0.1608,  0.1765,  0.1608],\n         [ 0.1608,  0.2000,  0.2078,  ...,  0.1137,  0.1294,  0.1843],\n         ...,\n         [ 0.3569,  0.2784,  0.1216,  ...,  0.1137,  0.1294,  0.1608],\n         [ 0.3569,  0.3333,  0.1686,  ...,  0.0118,  0.0196,  0.0667],\n         [ 0.3333,  0.2941,  0.1529,  ..., -0.1451, -0.0745,  0.0510]],\n\n        [[ 0.2078,  0.1843,  0.2000,  ...,  0.2392,  0.2471,  0.2314],\n         [ 0.2549,  0.2314,  0.2157,  ...,  0.2392,  0.2392,  0.2157],\n         [ 0.2471,  0.2863,  0.3098,  ...,  0.1922,  0.1922,  0.2314],\n         ...,\n         [ 0.3020,  0.3020,  0.2706,  ...,  0.4588,  0.4667,  0.4588],\n         [ 0.3098,  0.3569,  0.3255,  ...,  0.3176,  0.3020,  0.3020],\n         [ 0.2863,  0.3255,  0.3176,  ...,  0.0824,  0.1294,  0.2000]]]), 'labels': 0}"},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"train_size = int(len(train_dataset) * 0.8)\nval_size = len(train_dataset) - train_size\ntorch.manual_seed(42)\ntrain_dataset, val_dataset = torch.utils.data.random_split(train_dataset, [train_size, val_size])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-03T09:34:40.563404Z","iopub.execute_input":"2025-03-03T09:34:40.563736Z","iopub.status.idle":"2025-03-03T09:34:40.574189Z","shell.execute_reply.started":"2025-03-03T09:34:40.563706Z","shell.execute_reply":"2025-03-03T09:34:40.573494Z"}},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"## Fine-tuning the model","metadata":{}},{"cell_type":"code","source":"def collate_fn(batch):\n    return {\n        'pixel_values': torch.stack([x['pixel_values'] for x in batch]),\n        'labels': torch.tensor([x['labels'] for x in batch])\n    }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-03T09:35:11.489664Z","iopub.execute_input":"2025-03-03T09:35:11.489937Z","iopub.status.idle":"2025-03-03T09:35:11.494042Z","shell.execute_reply.started":"2025-03-03T09:35:11.489915Z","shell.execute_reply":"2025-03-03T09:35:11.493253Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score, f1_score\n\ndef compute_metrics(p):\n    preds = np.argmax(p.predictions, axis=1)\n    labels = p.label_ids\n    accuracy = accuracy_score(labels, preds)\n    f1score = f1_score(labels, preds, average='weighted')\n    return {'accuracy': accuracy, 'f1': f1score}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-03T09:35:13.744060Z","iopub.execute_input":"2025-03-03T09:35:13.744447Z","iopub.status.idle":"2025-03-03T09:35:13.749988Z","shell.execute_reply.started":"2025-03-03T09:35:13.744417Z","shell.execute_reply":"2025-03-03T09:35:13.749206Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"device = 'cuda' if torch.cuda.is_available() else 'cpu'\ndevice","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-03T09:35:14.969244Z","iopub.execute_input":"2025-03-03T09:35:14.969524Z","iopub.status.idle":"2025-03-03T09:35:15.022637Z","shell.execute_reply.started":"2025-03-03T09:35:14.969502Z","shell.execute_reply":"2025-03-03T09:35:15.021723Z"}},"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"'cuda'"},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"model = AutoModelForImageClassification.from_pretrained(model_path, num_labels=5, ignore_mismatched_sizes=True).to(device)\nmodel","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-03T09:35:18.247758Z","iopub.execute_input":"2025-03-03T09:35:18.248029Z","iopub.status.idle":"2025-03-03T09:35:21.183786Z","shell.execute_reply.started":"2025-03-03T09:35:18.248009Z","shell.execute_reply":"2025-03-03T09:35:21.183008Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/346M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f594c28a715f4a158a318c656bba5108"}},"metadata":{}},{"name":"stderr","text":"Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized because the shapes did not match:\n- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([5]) in the model instantiated\n- classifier.weight: found shape torch.Size([1000, 768]) in the checkpoint and torch.Size([5, 768]) in the model instantiated\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"ViTForImageClassification(\n  (vit): ViTModel(\n    (embeddings): ViTEmbeddings(\n      (patch_embeddings): ViTPatchEmbeddings(\n        (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n      )\n      (dropout): Dropout(p=0.0, inplace=False)\n    )\n    (encoder): ViTEncoder(\n      (layer): ModuleList(\n        (0-11): 12 x ViTLayer(\n          (attention): ViTSdpaAttention(\n            (attention): ViTSdpaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.0, inplace=False)\n            )\n            (output): ViTSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.0, inplace=False)\n            )\n          )\n          (intermediate): ViTIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): ViTOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (dropout): Dropout(p=0.0, inplace=False)\n          )\n          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        )\n      )\n    )\n    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n  )\n  (classifier): Linear(in_features=768, out_features=5, bias=True)\n)"},"metadata":{}}],"execution_count":12},{"cell_type":"code","source":"for i, layer in enumerate(model.children()):\n    if i != 1:\n        for param in layer.parameters():\n            param.requires_grad = False","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-03T09:36:25.829670Z","iopub.execute_input":"2025-03-03T09:36:25.829978Z","iopub.status.idle":"2025-03-03T09:36:25.834910Z","shell.execute_reply.started":"2025-03-03T09:36:25.829953Z","shell.execute_reply":"2025-03-03T09:36:25.833885Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"train_args = TrainingArguments(\n    num_train_epochs=3,\n    learning_rate=2e-4,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=16,\n    weight_decay=0.01,\n    warmup_steps=30, # 1920 сэмплов / 16 batch size = 120 шагов в эпохе\n    eval_strategy='epoch',\n    lr_scheduler_type='constant',\n    run_name='',\n    output_dir='/kaggle/working/output',\n    logging_dir='/kaggle/working/logs',\n    save_strategy='epoch',\n    logging_strategy='epoch',\n    disable_tqdm=False,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-03T09:36:28.697906Z","iopub.execute_input":"2025-03-03T09:36:28.698219Z","iopub.status.idle":"2025-03-03T09:36:28.729902Z","shell.execute_reply.started":"2025-03-03T09:36:28.698194Z","shell.execute_reply":"2025-03-03T09:36:28.729086Z"}},"outputs":[{"name":"stderr","text":"Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"trainer = Trainer(\n    model=model,\n    args=train_args,\n    tokenizer=processor,\n    data_collator=collate_fn,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    compute_metrics=compute_metrics\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-03T09:36:41.005648Z","iopub.execute_input":"2025-03-03T09:36:41.005921Z","iopub.status.idle":"2025-03-03T09:36:41.017322Z","shell.execute_reply.started":"2025-03-03T09:36:41.005901Z","shell.execute_reply":"2025-03-03T09:36:41.016569Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"trainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-03T09:36:48.156450Z","iopub.execute_input":"2025-03-03T09:36:48.156761Z","iopub.status.idle":"2025-03-03T09:38:44.894674Z","shell.execute_reply.started":"2025-03-03T09:36:48.156732Z","shell.execute_reply":"2025-03-03T09:38:44.893962Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='360' max='360' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [360/360 01:55, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>1.087900</td>\n      <td>0.696588</td>\n      <td>0.814583</td>\n      <td>0.814739</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.542300</td>\n      <td>0.482571</td>\n      <td>0.860417</td>\n      <td>0.859194</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.400900</td>\n      <td>0.394916</td>\n      <td>0.893750</td>\n      <td>0.893092</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=360, training_loss=0.6770513852437338, metrics={'train_runtime': 116.4108, 'train_samples_per_second': 49.48, 'train_steps_per_second': 3.092, 'total_flos': 4.463658617654477e+17, 'train_loss': 0.6770513852437338, 'epoch': 3.0})"},"metadata":{}}],"execution_count":18},{"cell_type":"markdown","source":"## Making prediction","metadata":{}},{"cell_type":"code","source":"model.eval()\n\ndef predict_sample(img_path):\n    image = Image.open(img_path)\n    item = processor(image, return_tensors='pt')\n    # item[\"pixel_values\"] = item[\"pixel_values\"].squeeze(0)\n    item = item.to(device)\n    with torch.no_grad():\n        outputs = model(**item)\n    prob = torch.nn.functional.softmax(outputs.logits, dim=1)\n    pred = torch.argmax(prob, dim=1).item()\n    return pred","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-03T09:38:44.895815Z","iopub.execute_input":"2025-03-03T09:38:44.896124Z","iopub.status.idle":"2025-03-03T09:38:44.901811Z","shell.execute_reply.started":"2025-03-03T09:38:44.896091Z","shell.execute_reply":"2025-03-03T09:38:44.900960Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"sub = pd.read_csv('/kaggle/input/image-classification/sample_submission.csv')\nsub.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-03T09:40:00.463803Z","iopub.execute_input":"2025-03-03T09:40:00.464234Z","iopub.status.idle":"2025-03-03T09:40:00.476375Z","shell.execute_reply.started":"2025-03-03T09:40:00.464186Z","shell.execute_reply":"2025-03-03T09:40:00.475303Z"}},"outputs":[{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"              name  label\n0  image_62214.jpg    NaN\n1  image_91562.jpg    NaN\n2  image_44104.jpg    NaN\n3  image_79943.jpg    NaN\n4  image_79847.jpg    NaN","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>name</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>image_62214.jpg</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>image_91562.jpg</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>image_44104.jpg</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>image_79943.jpg</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>image_79847.jpg</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":23},{"cell_type":"code","source":"labels = []\npaths = sub['name'].to_list()\nfor path in tqdm_notebook(paths):\n    labels.append(class_to_label[predict_sample('/kaggle/input/image-classification/test/test/' + path)])\nsub['label'] = labels\nsub.to_csv('sub.csv', index=False)\nsub.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-03T09:40:15.817946Z","iopub.execute_input":"2025-03-03T09:40:15.818327Z","iopub.status.idle":"2025-03-03T09:40:27.760485Z","shell.execute_reply.started":"2025-03-03T09:40:15.818287Z","shell.execute_reply":"2025-03-03T09:40:27.759618Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/600 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ea5572d5e6d74ef49d94331a23db3dea"}},"metadata":{}},{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"              name label\n0  image_62214.jpg  rose\n1  image_91562.jpg  rose\n2  image_44104.jpg  rose\n3  image_79943.jpg  rose\n4  image_79847.jpg  rose","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>name</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>image_62214.jpg</td>\n      <td>rose</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>image_91562.jpg</td>\n      <td>rose</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>image_44104.jpg</td>\n      <td>rose</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>image_79943.jpg</td>\n      <td>rose</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>image_79847.jpg</td>\n      <td>rose</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":24},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}